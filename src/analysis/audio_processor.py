import whisper
import torch
import json
import logging
import re
from pathlib import Path

logger = logging.getLogger(__name__)

class AudioProcessor:
    def __init__(self, model_size="medium"):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        if torch.backends.mps.is_available():
             self.device = "cpu" 
        
        logger.info(f"üëÇ Loading Whisper model ('{model_size}') on {self.device}...")
        self.model = whisper.load_model(model_size, device=self.device)

    def transcribe(self, audio_path):
        logger.info(f"üéô Transcribing {audio_path} (word-level)...")
        result = self.model.transcribe(str(audio_path), fp16=False, task="transcribe", word_timestamps=True)
        return result['segments']

    def syntax_segmentation(self, raw_segments):
        """
        –†–µ–∂–µ—Ç –ø–æ –∑–Ω–∞–∫–∞–º –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è (.,!?:;-), –Ω–æ –Ω–µ —á–∞—â–µ —á–µ–º —Ä–∞–∑ –≤ 3 —Å–ª–æ–≤–∞.
        –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç GAPLESS (–æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –¥—ã—Ä).
        """
        logger.info("üîß Segmenting by Punctuation + Gapless Flow...")
        
        all_words = []
        for seg in raw_segments:
            if 'words' in seg:
                all_words.extend(seg['words'])
        
        if not all_words: return []

        new_segments = []
        current_words = []
        
        # –ò—â–µ–º –ª—é–±—ã–µ –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
        punct_pattern = re.compile(r"[.,!?;:-]$")
        
        segment_start = 0.0
        
        for i, word_data in enumerate(all_words):
            current_words.append(word_data)
            word_text = word_data['word'].strip()
            
            has_punct = bool(punct_pattern.search(word_text))
            enough_words = len(current_words) >= 3
            is_last_global = (i == len(all_words) - 1)

            if (has_punct and enough_words) or is_last_global:
                
                # Gapless –ª–æ–≥–∏–∫–∞
                if is_last_global:
                    segment_end = word_data['end']
                else:
                    segment_end = all_words[i+1]['start']

                text = "".join([w['word'] for w in current_words]).strip()
                
                new_segments.append({
                    "start": segment_start,
                    "end": segment_end,
                    "duration": segment_end - segment_start,
                    "text": text
                })
                
                segment_start = segment_end
                current_words = []

        # –•–≤–æ—Å—Ç (–Ω–∞ —Å–ª—É—á–∞–π —Å–±–æ—è –ª–æ–≥–∏–∫–∏)
        if current_words:
            text = "".join([w['word'] for w in current_words]).strip()
            new_segments.append({
                "start": segment_start,
                "end": current_words[-1]['end'],
                "duration": current_words[-1]['end'] - segment_start,
                "text": text
            })

        return new_segments

    def create_batches(self, segments, target_context_words=60):
        """
        –ì—Ä—É–ø–ø–∏—Ä—É–µ—Ç —Å–µ–≥–º–µ–Ω—Ç—ã –≤ –±–∞—Ç—á–∏.
        –í–ê–ñ–ù–û: –ó–∞–∫—Ä—ã–≤–∞–µ—Ç –±–∞—Ç—á –¢–û–õ–¨–ö–û –µ—Å–ª–∏ –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è —Ç–æ—á–∫–æ–π/–≤–æ—Å–∫–ª/–≤–æ–ø—Ä–æ—Å–æ–º.
        """
        batches = []
        current_batch_segments = []
        current_word_count = 0
        batch_id = 0

        # –†–µ–≥—É–ª—è—Ä–∫–∞ –¥–ª—è "—Å–∏–ª—å–Ω–æ–≥–æ" –∫–æ–Ω—Ü–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (Sentence End)
        # –ò—â–µ–º . ! ? –≤ –∫–æ–Ω—Ü–µ —Å—Ç—Ä–æ–∫–∏ (–≤–æ–∑–º–æ–∂–Ω–æ –ø–µ—Ä–µ–¥ –∫–∞–≤—ã—á–∫–æ–π)
        sentence_end_pattern = re.compile(r"[.!?][\"']?$")

        for idx, seg in enumerate(segments):
            # –ü—Ä–∏—Å–≤–∞–∏–≤–∞–µ–º ID —Å–µ–≥–º–µ–Ω—Ç—É –≥–ª–æ–±–∞–ª—å–Ω–æ (–≤–∞–∂–Ω–æ –¥–ª—è —Å–≤—è–∑–∫–∏ —Å EDL)
            seg['segment_id'] = idx
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Ç–µ–∫—É—â–∏–π –±–∞—Ç—á
            current_batch_segments.append(seg)
            current_word_count += len(seg['text'].split())
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ—Ç —Å–µ–≥–º–µ–Ω—Ç –∫–æ–Ω—Ü–æ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            text = seg['text'].strip()
            is_sentence_end = bool(sentence_end_pattern.search(text))
            
            # –õ–û–ì–ò–ö–ê –ó–ê–ö–†–´–¢–ò–Ø –ë–ê–¢–ß–ê:
            # 1. –ù–∞–±—Ä–∞–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–ª–æ–≤ (target_context_words)
            # 2. –ò (–û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û) —Ç–µ–∫—É—â–∏–π —Å–µ–≥–º–µ–Ω—Ç –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
            if current_word_count >= target_context_words and is_sentence_end:
                full_text = " ".join([s['text'] for s in current_batch_segments])
                batches.append({
                    "batch_id": batch_id,
                    "context_text": full_text,
                    "segments": current_batch_segments
                })
                
                batch_id += 1
                current_batch_segments = []
                current_word_count = 0
        
        # –ï—Å–ª–∏ –æ—Å—Ç–∞–ª–∏—Å—å —Å–µ–≥–º–µ–Ω—Ç—ã (—Ö–≤–æ—Å—Ç, –¥–∞–∂–µ –µ—Å–ª–∏ –Ω–µ –∫–æ–Ω—á–∞–µ—Ç—Å—è —Ç–æ—á–∫–æ–π)
        if current_batch_segments:
            full_text = " ".join([s['text'] for s in current_batch_segments])
            batches.append({
                "batch_id": batch_id,
                "context_text": full_text,
                "segments": current_batch_segments
            })

        return batches

    def process(self, audio_path, output_path):
        # 1. Whisper
        raw = self.transcribe(audio_path)
        
        # 2. Syntax Cut + Gapless
        optimized = self.syntax_segmentation(raw)
        
        # 3. Smart Batching (Sentence Aware)
        batches = self.create_batches(optimized)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(batches, f, indent=2, ensure_ascii=False)
            
        logger.info(f"‚úÖ Transcript ready: {len(optimized)} segments. Syntax-aware batching applied.")
        return batches