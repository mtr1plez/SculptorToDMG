import json
import logging
import torch
import clip
import numpy as np
from pathlib import Path
from tqdm import tqdm

logger = logging.getLogger(__name__)

class SmartMatcher:
    def __init__(self, library_path, model_name="ViT-B/32"):
        self.library_path = Path(library_path)
        # CLIP –¥–ª—è —Ç–µ–∫—Å—Ç–∞ –æ—á–µ–Ω—å –ª–µ–≥–∫–∏–π, CPU —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è –º–≥–Ω–æ–≤–µ–Ω–Ω–æ
        self.device = "cpu" 
        
        logger.info(f"üß† Loading CLIP model for matching...")
        self.model, _ = clip.load(model_name, device=self.device)
        
        # –ö—ç—à –¥–ª—è –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Ñ–∏–ª—å–º–æ–≤
        self.loaded_sources = {}

    def _load_source(self, source_name):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏–Ω–¥–µ–∫—Å –∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Ñ–∏–ª—å–º–∞ –≤ –ø–∞–º—è—Ç—å."""
        if source_name in self.loaded_sources:
            return self.loaded_sources[source_name]

        source_dir = self.library_path / source_name
        index_path = source_dir / "master_index.json"
        emb_path = source_dir / "embeddings.npy"

        if not index_path.exists() or not emb_path.exists():
            logger.error(f"‚ùå Missing index/embeddings for {source_name}")
            return None

        logger.info(f"üìÇ Loading source data: {source_name}")
        with open(index_path, 'r') as f:
            index_data = json.load(f)
        
        # === –ü–û–î–î–ï–†–ñ–ö–ê –ù–û–í–û–ì–û –ò –°–¢–ê–†–û–ì–û –§–û–†–ú–ê–¢–ê ===
        # –ù–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç: {"movie_name": "...", "source_video_path": "...", "scenes": [...]}
        # –°—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç: –ø—Ä–æ—Å—Ç–æ —Å–ø–∏—Å–æ–∫ —Å—Ü–µ–Ω [...]
        if isinstance(index_data, dict) and "scenes" in index_data:
            # –ù–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç
            master_index = index_data["scenes"]
            source_video_path = index_data.get("source_video_path")
        else:
            # –°—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç (–æ–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å)
            master_index = index_data
            source_video_path = None
            logger.warning(f"‚ö†Ô∏è {source_name} uses old index format (no video path). Consider re-indexing.")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–ª–æ–≤–∞—Ä—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        embeddings = np.load(emb_path, allow_pickle=True).item() 
        
        ordered_vectors = []
        valid_scenes = []
        
        # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ —Å—Ü–µ–Ω —Å –º–∞—Ç—Ä–∏—Ü–µ–π –≤–µ–∫—Ç–æ—Ä–æ–≤
        for scene in master_index:
            s_id = scene['id']
            if s_id in embeddings:
                ordered_vectors.append(embeddings[s_id])
                valid_scenes.append(scene)
        
        if not ordered_vectors:
            return None
            
        # –°–æ–∑–¥–∞–µ–º –º–∞—Ç—Ä–∏—Ü—É –ø–æ–∏—Å–∫–∞
        matrix = np.array(ordered_vectors) 
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è NumPy (–∑–¥–µ—Å—å keepdims –≤–æ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–º —á–∏—Å–ª–µ - –≤–µ—Ä–Ω–æ)
        norm = np.linalg.norm(matrix, axis=1, keepdims=True)
        matrix = matrix / (norm + 1e-8)

        # –ü–µ—Ä–µ–≤–æ–¥–∏–º –≤ PyTorch Tensor –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —É–º–Ω–æ–∂–µ–Ω–∏—è
        data = {
            "scenes": valid_scenes,
            "matrix": torch.from_numpy(matrix).float().to(self.device),
            "source_name": source_name,
            "source_video_path": source_video_path  # <--- –î–û–ë–ê–í–ò–õ–ò –ü–£–¢–¨ –ö –í–ò–î–ï–û
        }
        self.loaded_sources[source_name] = data
        return data

    def match(self, script_path, output_path, source_names):
        script_path = Path(script_path)
        with open(script_path, 'r') as f:
            script = json.load(f)

        # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        active_sources = []
        for src in source_names:
            data = self._load_source(src)
            if data: active_sources.append(data)
            
        if not active_sources:
            logger.error("No valid sources loaded!")
            return

        final_edl = [] 
        used_scene_ids = set() # –°–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ü–µ–Ω –¥–ª—è –∑–∞—â–∏—Ç—ã –æ—Ç –ø–æ–≤—Ç–æ—Ä–æ–≤

        logger.info(f"üéØ Matching {len(script)} segments...")

        for segment in tqdm(script, desc="Matching"):
            query_text = segment.get("visual_query", "")
            target_char = segment.get("character")
            target_shot = segment.get("shot_type")
            
            # 2. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ —ç–Ω–∫–æ–¥–∏–Ω–≥ —Ç–µ–∫—Å—Ç–∞
            text_token = clip.tokenize([query_text], truncate=True).to(self.device)
            
            with torch.no_grad():
                text_emb = self.model.encode_text(text_token).float()
                
                # !!! –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –í PyTorch –∞—Ä–≥—É–º–µ–Ω—Ç –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è keepdim (–µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ —á–∏—Å–ª–æ) !!!
                text_emb /= text_emb.norm(dim=-1, keepdim=True)

            best_score = -10000
            best_match = None
            best_source = None

            # 3. –ü–æ–∏—Å–∫ –ø–æ –≤—Å–µ–º —Ñ–∏–ª—å–º–∞–º
            for src_data in active_sources:
                # –ú–∞—Ç—Ä–∏—á–Ω–æ–µ —É–º–Ω–æ–∂–µ–Ω–∏–µ (Cosine Similarity)
                # –†–µ–∑—É–ª—å—Ç–∞—Ç: –º–∞—Å—Å–∏–≤ —Å—Ö–æ–∂–µ—Å—Ç–∏ –¥–ª—è –≤—Å–µ—Ö —Å—Ü–µ–Ω —Å—Ä–∞–∑—É
                sim_scores = (text_emb @ src_data["matrix"].T).squeeze(0).cpu().numpy()

                for idx, scene in enumerate(src_data["scenes"]):
                    s_id = scene['id']
                    
                    # –ë–∞–∑–æ–≤—ã–π —Å–∫–æ—Ä –æ—Ç CLIP (–æ–±—ã—á–Ω–æ –æ—Ç 15 –¥–æ 35)
                    score = sim_scores[idx] * 100.0 

                    # --- –°–ò–°–¢–ï–ú–ê –§–ò–õ–¨–¢–†–û–í –ò –®–¢–†–ê–§–û–í ---

                    # A. –§–∏–ª—å—Ç—Ä –ü–µ—Ä—Å–æ–Ω–∞–∂–∞ (–°–∞–º—ã–π –≤–∞–∂–Ω—ã–π)
                    scene_chars = scene["content"].get("characters", [])
                    
                    if target_char:
                        # –ï—Å–ª–∏ –∏—â–µ–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –≥–µ—Ä–æ—è
                        if target_char in scene_chars:
                            score += 500 # –û–≥—Ä–æ–º–Ω—ã–π –±–æ–Ω—É—Å, –µ—Å–ª–∏ –Ω–∞—à–ª–∏
                        else:
                            score -= 500 # –û–≥—Ä–æ–º–Ω—ã–π —à—Ç—Ä–∞—Ñ, –µ—Å–ª–∏ –≥–µ—Ä–æ—è –Ω–µ—Ç
                    else:
                        # –ï—Å–ª–∏ –∏—â–µ–º B-Roll (–ø–µ–π–∑–∞–∂, –¥–µ—Ç–∞–ª—å), –∞ –≤ –∫–∞–¥—Ä–µ –≥–µ—Ä–æ–∏
                        if scene_chars: 
                            score -= 50 # –ù–µ–±–æ–ª—å—à–æ–π —à—Ç—Ä–∞—Ñ, –ª—É—á—à–µ –Ω–∞–π—Ç–∏ –ø—É—Å—Ç–æ–π –∫–∞–¥—Ä

                    # B. –§–∏–ª—å—Ç—Ä –¢–∏–ø–∞ –ö–∞–¥—Ä–∞ (Close-Up, Wide...)
                    scene_shot = scene["visual"].get("shot_type", "Unknown")
                    if target_shot and scene_shot == target_shot:
                        score += 50 # –ë–æ–Ω—É—Å –∑–∞ –ø—Ä–∞–≤–∏–ª—å–Ω—É—é –∫—Ä—É–ø–Ω–æ—Å—Ç—å –ø–ª–∞–Ω–∞
                    
                    # C. –ó–∞—â–∏—Ç–∞ –æ—Ç –ø–æ–≤—Ç–æ—Ä–æ–≤
                    if s_id in used_scene_ids:
                        score -= 10000 # –ó–∞–ø—Ä–µ—â–∞–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—Ü–µ–Ω—É –ø–æ–≤—Ç–æ—Ä–Ω–æ

                    # –ó–∞–ø–æ–º–∏–Ω–∞–µ–º –ª–∏–¥–µ—Ä–∞
                    if score > best_score:
                        best_score = score
                        best_match = scene
                        best_source = src_data["source_name"]

            # 4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            if best_match:
                used_scene_ids.add(best_match['id'])
                best_source_data = next((s for s in active_sources if s["source_name"] == best_source), None)
                video_path = best_source_data["source_video_path"] if best_source_data else None
                
                edit_entry = {
                    "segment_id": segment.get("segment_id", 0),
                    "text": segment.get("text"),
                    # –ü—É—Ç—å –∫ –∫–∞—Ä—Ç–∏–Ω–∫–µ (–¥–ª—è –¥–µ–±–∞–≥–∞)
                    "source_file": best_match["visual"]["path"], 
                    "source_project_alias": best_source,
                    "source_video_path": video_path,
                    "scene_id": best_match['id'],
                    # –¢–∞–π–º–∫–æ–¥—ã
                    "in_point": best_match['time']['start'],
                    "out_point": best_match['time']['end'],
                    "duration": best_match['time']['end'] - best_match['time']['start'],
                    "target_duration": segment.get('target_duration', segment['end'] - segment['start']),
                    # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                    "match_score": float(best_score),
                    "shot_type": best_match["visual"]["shot_type"],
                    "characters": best_match["content"]["characters"]
                }
                final_edl.append(edit_entry)
            else:
                logger.warning(f"‚ö†Ô∏è No match found for segment: {segment.get('text', '')[:20]}...")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ JSON
        with open(output_path, 'w') as f:
            json.dump(final_edl, f, indent=2)
            
        logger.info(f"‚úÖ Created Edit Decision List with {len(final_edl)} cuts: {output_path}")