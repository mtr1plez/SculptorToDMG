import os
import cv2
import numpy as np
import json
import logging
from pathlib import Path
from tqdm import tqdm
import pickle

from insightface.app import FaceAnalysis
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import normalize

logger = logging.getLogger(__name__)

class FaceProcessor:
    def __init__(self, output_dir):
        self.output_dir = Path(output_dir)
        self.keyframes_dir = self.output_dir / "keyframes"
        self.faces_path = self.output_dir / "faces_clusters.json"
        self.face_reps_path = self.output_dir / "face_representatives.json"

        self.app = None 

    def _load_model(self):
        if self.app is None:
            logger.info("‚ö°Ô∏è Loading LIGHTWEIGHT InsightFace model (buffalo_s)...")
            # buffalo_s - —Å—É–ø–µ—Ä-–±—ã—Å—Ç—Ä–∞—è –º–æ–¥–µ–ª—å. –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∏–∂–µ, –Ω–æ —Å–∫–æ—Ä–æ—Å—Ç—å —Ö10.
            # det_size=(640, 640) - —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ.
            self.app = FaceAnalysis(name='buffalo_s', providers=['CPUExecutionProvider'])
            self.app.prepare(ctx_id=0, det_size=(640, 640))

    def process_faces(self):
        # SKIP LOGIC
        if self.faces_path.exists() and self.face_reps_path.exists():
            logger.info(f"‚è≠Ô∏è  Face data exists. Skipping.")
            return

        if not self.keyframes_dir.exists():
            logger.error(f"Keyframes dir not found.")
            return

        self._load_model()

        image_files = sorted(list(self.keyframes_dir.glob("*.jpg")))
        logger.info(f"üîç Scanning faces in {len(image_files)} keyframes...")

        all_embeddings = []
        embedding_map = [] 

        # --- –≠–¢–ê–ü 1: –°—Ç—Ä–æ–≥–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è ---
        detected_count = 0
        skipped_low_quality = 0

        for img_path in tqdm(image_files, desc="Detecting Faces"):
            img = cv2.imread(str(img_path))
            if img is None: continue
            
            try:
                faces = self.app.get(img)
            except Exception: continue
            
            if not faces: continue

            scene_id = "_".join(img_path.stem.split("_")[:-1])

            for face in faces:
                # –û–ß–ï–ù–¨ –í–ê–ñ–ù–û: –ü–æ–¥–Ω–∏–º–∞–µ–º –ø–æ—Ä–æ–≥ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–æ 0.60
                # –ú—ã –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Ä–∞–∑–º—ã—Ç—ã–µ –ª–∏—Ü–∞, –∫–æ—Ç–æ—Ä—ã–µ —Å–ª—É–∂–∞—Ç "–º–æ—Å—Ç–∏–∫–æ–º" –¥–ª—è —Å–∫–ª–µ–∏–≤–∞–Ω–∏—è —Ä–∞–∑–Ω—ã—Ö –ª—é–¥–µ–π.
                if face.det_score < 0.60: 
                    skipped_low_quality += 1
                    continue 

                all_embeddings.append(face.embedding)
                embedding_map.append({
                    "scene_id": scene_id,
                    "filename": img_path.name,
                    "score": float(face.det_score)
                })
                detected_count += 1

        logger.info(f"üìä Faces detected: {detected_count}. Skipped blur/bad: {skipped_low_quality}")
        
        if detected_count == 0:
            logger.warning("‚ö†Ô∏è No high-quality faces found! Try lowering threshold slightly.")
            with open(self.faces_path, 'w') as f: json.dump({}, f)
            with open(self.face_reps_path, 'w') as f: json.dump({}, f)
            return

        # --- –≠–¢–ê–ü 2: –î—Ä–æ–±–ª–µ–Ω–∏–µ –ö–ª–∞—Å—Ç–µ—Ä–æ–≤ ---
        logger.info("Pre-normalizing embeddings...")
        X = normalize(np.array(all_embeddings))

        logger.info("üß© Clustering faces (Fragmentation Mode)...")
        
        # eps=0.40 -> –≠–ö–°–¢–†–ï–ú–ê–õ–¨–ù–û –Ω–∏–∑–∫–∏–π –ø–æ—Ä–æ–≥.
        # –≠—Ç–æ –∑–∞—Å—Ç–∞–≤–∏—Ç –∞–ª–≥–æ—Ä–∏—Ç–º —Å—á–∏—Ç–∞—Ç—å "–æ–¥–Ω–∏–º –ª–∏—Ü–æ–º" —Ç–æ–ª—å–∫–æ –ø–æ—á—Ç–∏ –∏–¥–µ–Ω—Ç–∏—á–Ω—ã–µ —Ñ–æ—Ç–∫–∏.
        # –ú–∏–∫–∞—ç–ª—å —Ä–∞–∑–æ–±—å–µ—Ç—Å—è –Ω–∞ 3-4 —Ä–∞–∑–Ω—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–∞, –Ω–æ –∑–∞—Ç–æ –æ–Ω –ù–ï —Å–∫–ª–µ–∏—Ç—Å—è —Å –õ–∏—Å–±–µ—Ç.
        clustering = DBSCAN(eps=0.40, min_samples=2, metric="cosine", n_jobs=-1).fit(X)
        
        labels = clustering.labels_
        
        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
        logger.info(f"üî¢ Statistics: {n_clusters} clusters (fragments) found.")

        # --- –≠–¢–ê–ü 3: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ ---
        scene_faces = {} 
        representative_faces = {} 

        for idx, label in enumerate(labels):
            if label == -1: continue 
            
            person_id = f"person_{label}"
            data = embedding_map[idx]
            s_id = data["scene_id"]

            if s_id not in scene_faces: scene_faces[s_id] = set()
            scene_faces[s_id].add(person_id)
            
            if person_id not in representative_faces:
                representative_faces[person_id] = {"path": data["filename"], "score": data["score"]}
            else:
                if data["score"] > representative_faces[person_id]["score"]:
                    representative_faces[person_id] = {"path": data["filename"], "score": data["score"]}

        with open(self.face_reps_path, 'w') as f:
            json.dump(representative_faces, f, indent=2)

        final_json = {k: list(v) for k, v in scene_faces.items()}
        with open(self.faces_path, 'w') as f:
            json.dump(final_json, f, indent=2)

        logger.info(f"üíæ Saved data to {self.faces_path}")