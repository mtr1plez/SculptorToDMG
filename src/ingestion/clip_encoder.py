import torch
import clip
import os
import numpy as np
import json
import logging
from PIL import Image
from pathlib import Path
from tqdm import tqdm

logger = logging.getLogger(__name__)

# –¢–∏–ø—ã –∫–∞–¥—Ä–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –º—ã —Ö–æ—Ç–∏–º —Ä–∞–∑–ª–∏—á–∞—Ç—å
SHOT_TYPES = [
    "Extreme Close-Up",   # –ì–ª–∞–∑, –¥–µ—Ç–∞–ª—å, –ø–∞–ª–µ—Ü
    "Close-Up Face",      # –õ–∏—Ü–æ –Ω–∞ –≤–µ—Å—å —ç–∫—Ä–∞–Ω
    "Medium Shot",        # –ü–æ –ø–æ—è—Å
    "Two Shot",           # –î–≤–∞ —á–µ–ª–æ–≤–µ–∫–∞
    "Wide Angle",         # –û–±—â–∏–π –ø–ª–∞–Ω (–∫–æ–º–Ω–∞—Ç–∞, —É–ª–∏—Ü–∞)
    "Scenery / Landscape" # –ü–µ–π–∑–∞–∂ –±–µ–∑ –ª—é–¥–µ–π
]

class ClipEncoder:
    def __init__(self, output_dir, model_name="ViT-B/32"):
        self.output_dir = Path(output_dir)
        self.keyframes_dir = self.output_dir / "keyframes"
        self.embeddings_path = self.output_dir / "embeddings.npy"
        self.visual_tags_path = self.output_dir / "visual_tags.json"
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (Apple Silicon MPS –∏–ª–∏ CPU)
        if torch.backends.mps.is_available():
            self.device = "mps"
        elif torch.cuda.is_available():
            self.device = "cuda"
        else:
            self.device = "cpu"
            
        logger.info(f"üëÅ Loading CLIP model ({model_name}) on {self.device}...")
        self.model, self.preprocess = clip.load(model_name, device=self.device)
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –≤–µ–∫—Ç–æ—Ä—ã –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–∏–ø–∞ –∫–∞–¥—Ä–∞
        logger.info("üìê Pre-calculating shot type vectors...")
        text_inputs = clip.tokenize(SHOT_TYPES).to(self.device)
        with torch.no_grad():
            self.shot_type_features = self.model.encode_text(text_inputs)
            self.shot_type_features /= self.shot_type_features.norm(dim=-1, keepdim=True)

    def process_embeddings(self):
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤–µ–∫—Ç–æ—Ä—ã –¥–ª—è –∫–∞—Ä—Ç–∏–Ω–æ–∫ –∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –∫–∞–¥—Ä–∞.
        """
        if not self.keyframes_dir.exists():
            logger.error(f"Keyframes dir not found: {self.keyframes_dir}")
            return

        image_files = sorted(list(self.keyframes_dir.glob("*.jpg")))
        logger.info(f"üëÅ Encoding {len(image_files)} keyframes...")

        embeddings_dict = {} # scene_id -> [vector_start, vector_mid, vector_end]
        visual_tags = {}     # scene_id -> {"shot_type": "Close-Up", "probs": ...}

        # –ß—Ç–æ–±—ã –Ω–µ –≥—Ä—É–∑–∏—Ç—å –ø–∞–º—è—Ç—å, –ø—Ä–æ—Ü–µ—Å—Å–∏–º –ø–æ –æ–¥–Ω–æ–π –∫–∞—Ä—Ç–∏–Ω–∫–µ (CLIP –±—ã—Å—Ç—Ä—ã–π)
        # –ú–æ–∂–Ω–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –±–∞—Ç—á–∞–º–∏, –Ω–æ –¥–ª—è –Ω–∞—á–∞–ª–∞ —Ç–∞–∫ –±–µ–∑–æ–ø–∞—Å–Ω–µ–µ
        for img_path in tqdm(image_files, desc="CLIP Encoding"):
            try:
                # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥
                image = self.preprocess(Image.open(img_path)).unsqueeze(0).to(self.device)
                
                # 2. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
                with torch.no_grad():
                    image_features = self.model.encode_image(image)
                    
                    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–∞ (–≤–∞–∂–Ω–æ –¥–ª—è cosine similarity)
                    image_features /= image_features.norm(dim=-1, keepdim=True)

                    # 3. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –∫–∞–¥—Ä–∞ (Shot Classification)
                    # –°—á–∏—Ç–∞–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å –∫–∞—Ä—Ç–∏–Ω–∫–∏ —Å —Ç–µ–∫—Å—Ç–æ–≤—ã–º–∏ –æ–ø–∏—Å–∞–Ω–∏—è–º–∏ –ø–ª–∞–Ω–æ–≤
                    similarity = (100.0 * image_features @ self.shot_type_features.T).softmax(dim=-1)
                    values, indices = similarity[0].topk(1)
                    
                    best_shot_type = SHOT_TYPES[indices[0]]

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ
                # –ü–æ–ª—É—á–∞–µ–º scene_id –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ (scene_0001_0.jpg -> scene_0001)
                scene_id = "_".join(img_path.stem.split("_")[:-1])
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ–∫—Ç–æ—Ä –∫–∞–∫ numpy array (–ø–µ—Ä–µ–≤–æ–¥–∏–º –Ω–∞ CPU)
                vec_numpy = image_features.cpu().numpy()[0]
                
                if scene_id not in embeddings_dict:
                    embeddings_dict[scene_id] = []
                    visual_tags[scene_id] = {"shot_counts": {}}

                embeddings_dict[scene_id].append(vec_numpy)
                
                # –°—á–∏—Ç–∞–µ–º –≥–æ–ª–æ—Å–∞ –∑–∞ —Ç–∏–ø –∫–∞–¥—Ä–∞ (—É –Ω–∞—Å 3 –∫–∞–¥—Ä–∞ –Ω–∞ —Å—Ü–µ–Ω—É)
                # –ï—Å–ª–∏ 2 –∏–∑ 3 –∫–∞–¥—Ä–æ–≤ –≥–æ–≤–æ—Ä—è—Ç Close-Up, –∑–Ω–∞—á–∏—Ç —ç—Ç–æ Close-Up
                current_counts = visual_tags[scene_id]["shot_counts"]
                current_counts[best_shot_type] = current_counts.get(best_shot_type, 0) + 1

            except Exception as e:
                logger.error(f"Error processing {img_path}: {e}")

        # –§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
        final_embeddings = {}
        final_tags = {}

        for scene_id, vectors in embeddings_dict.items():
            # 1. –£—Å—Ä–µ–¥–Ω—è–µ–º –≤–µ–∫—Ç–æ—Ä —Å—Ü–µ–Ω—ã (–±–µ—Ä–µ–º —Å—Ä–µ–¥–Ω–µ–µ –º–µ–∂–¥—É 3 –∫–∞–¥—Ä–∞–º–∏)
            # –≠—Ç–æ –¥–∞–µ—Ç –±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω—ã–π –≤–µ–∫—Ç–æ—Ä –¥–ª—è –ø–æ–∏—Å–∫–∞
            avg_vector = np.mean(vectors, axis=0)
            final_embeddings[scene_id] = avg_vector

            # 2. –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏—Ç–æ–≥–æ–≤—ã–π —Ç–∏–ø —Å—Ü–µ–Ω—ã (Majority Vote)
            counts = visual_tags[scene_id]["shot_counts"]
            most_frequent_shot = max(counts, key=counts.get)
            final_tags[scene_id] = most_frequent_shot

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –Ω–∞ –¥–∏—Å–∫
        np.save(self.embeddings_path, final_embeddings)
        with open(self.visual_tags_path, 'w') as f:
            json.dump(final_tags, f, indent=2)

        logger.info(f"üíæ Embeddings saved to: {self.embeddings_path}")
        logger.info(f"üíæ Visual tags saved to: {self.visual_tags_path}")